{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":4034417,"sourceType":"datasetVersion","datasetId":2390305}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**<h1 align = \"center\">Toxic Comment Detection - Machine Learning</h1>**","metadata":{}},{"cell_type":"markdown","source":"## **INITIAL DATA ANALYSIS**<a id=\"8\"></a>","metadata":{}},{"cell_type":"markdown","source":"### **2.1 Loading Our Datasets**<a id=\"3\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntoxic_df = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\")","metadata":{"execution":{"iopub.status.busy":"2024-12-04T09:17:39.782745Z","iopub.execute_input":"2024-12-04T09:17:39.783247Z","iopub.status.idle":"2024-12-04T09:17:43.302027Z","shell.execute_reply.started":"2024-12-04T09:17:39.783202Z","shell.execute_reply":"2024-12-04T09:17:43.300611Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### **2.2 Initial Analysis On Our Datasets**<a id=\"4\"></a>","metadata":{}},{"cell_type":"code","source":"toxic_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"toxic_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2.3 Selecting The Required Columns**<a id=\"5\"></a>","metadata":{}},{"cell_type":"code","source":"#We are going to select just the \"comment_text\" and \"toxic\" columns\ntoxic_df['Toxic'] = toxic_df.iloc[:, 2:].any(axis = 1)\nselected_toxic_columns = toxic_df[['comment_text', 'Toxic']]\nselected_toxic_columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_toxic_columns.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_toxic_columns.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2.5 Handling Duplicates**<a id=\"7\"></a>","metadata":{}},{"cell_type":"code","source":"#Checking duplicates\nselected_toxic_columns.duplicated(subset = ['comment_text'], keep = False).sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Printing the duplicated rows\nduplicates = selected_toxic_columns[selected_toxic_columns.duplicated(subset = ['comment_text'], keep = False)]\nduplicates","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Dropping Duplicates\nselected_toxic_columns.drop_duplicates(subset = ['comment_text'], keep = 'first', inplace = True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Confirm Drops\nselected_toxic_columns.duplicated(subset = ['comment_text'], keep = False).sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_toxic_columns['Toxic'].value_counts()\n#We can see from the code above that the data is imbalanced.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **VISUALIZATION**<a id=\"8\"></a>","metadata":{}},{"cell_type":"markdown","source":"### **3.1 Toxic vs Non-Toxic Comments Plot**<a id=\"9\"></a>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n#Graphical representation of the Toxic column values (Toxic vs Non-Toxic Comments) distribution\nplt.figure(figsize = (6, 4))\ntoxic_counts = selected_toxic_columns['Toxic'].value_counts()\ntoxic_counts.plot(kind = 'bar', color = ['green', 'red'])\nplt.title('Toxic vs Non-Toxic Comments')\nplt.xlabel('Toxic')\nplt.ylabel('Count')\nplt.xticks(rotation = 0)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3.2 Wordcloud for Toxic Comments**<a id=\"10\"></a>","metadata":{}},{"cell_type":"code","source":"#\"Wordcloud\" is for creating word cloud visualization.\nfrom wordcloud import WordCloud\n#Creating Word Cloud of Toxic Comments\ntoxic_comments = ''.join(selected_toxic_columns[selected_toxic_columns['Toxic']]['comment_text'])\ntoxic_words = WordCloud(width = 900, height = 450, background_color = \"white\").generate(toxic_comments)\nplt.imshow(toxic_words, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.title(\"Word Cloud For Toxic Comments\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3.3 Wordcloud for Non-Toxic Comments**<a id=\"11\"></a>","metadata":{}},{"cell_type":"code","source":"#Creating Word Cloud of Non-Toxic Comments\nnon_toxic_comments = ''.join(selected_toxic_columns[~selected_toxic_columns['Toxic']]['comment_text'])\nnon_toxic_words = WordCloud(width = 900, height = 450, background_color = \"white\").generate(non_toxic_comments)\nplt.imshow(non_toxic_words, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.title(\"Word Cloud For Non-Toxic Comments\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **EXPLORATORY DATA ANALYSIS (EDA)**<a id=\"12\"></a>","metadata":{}},{"cell_type":"markdown","source":"### **4.1 Replacing True and False Values**<a id=\"13\"></a>","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n#Replacing True as 1 and False as 0. \nselected_toxic_columns['Toxic'] = selected_toxic_columns['Toxic'].replace({True: 1, False: 0})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.2 Text Preprocessing**<a id=\"14\"></a>","metadata":{}},{"cell_type":"code","source":"#\"re\" is for regular expressions and text processing.\nimport re\n#Cleaning the comment texts\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re'\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(\"\\W\", \" \", text)\n    text = re.sub(\"\\s+\", \" \", text)\n    text = text.strip(\" \")\n    \n    return text\n\nselected_toxic_columns['comment_text'] = selected_toxic_columns['comment_text'].map(lambda cleaned : clean_text(cleaned))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_toxic_columns.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.3 Text Processing Using TF-IDF**<a id=\"15\"></a>","metadata":{}},{"cell_type":"code","source":"\"\"\"TF-IDF(Term Frequency-Inverse Document Frequency) is used for text analysis: \nText to Numerical Conversion, Feature Extraction, Dimensionality Reduction, Normalization & Scaling etc.\"\"\"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvector = TfidfVectorizer(max_features = 5000, stop_words = 'english')\nX = vector.fit_transform(selected_toxic_columns['comment_text'])\nY = selected_toxic_columns['Toxic']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.4 Over-Sampling Using SMOTE**<a id=\"16\"></a>","metadata":{}},{"cell_type":"code","source":"selected_toxic_columns['Toxic'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Recall that the data is imbalanced, so we have to balance it using SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n#Initialize SMOTE\nsmote = SMOTE()\n\n#Using SMOTE for oversampling\nX_resampled, y_resampled = smote.fit_resample(X, Y)\n\n#Converting oversampled data to DataFrame\nresampled_df = pd.DataFrame(X_resampled.todense(), columns = vector.get_feature_names_out())\nresampled_df['Toxic'] = y_resampled","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resampled_df['Toxic'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plotting the new distribution sample\nplt.figure(figsize = (6, 4))\ntoxic_counts = resampled_df['Toxic'].value_counts()\ntoxic_counts.plot(kind = 'bar', color = ['green', 'red'])\nplt.title('Toxic vs Non-Toxic Comments')\nplt.xlabel('Toxic')\nplt.ylabel('Count')\nplt.xticks(rotation = 0)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **MODELLING**<a id=\"17\"></a>","metadata":{}},{"cell_type":"markdown","source":"### **5.1 Splitting Our Dataset**<a id=\"18\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#Splitting the New Dataset into Training and Testing\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5.2 Building Model**<a id=\"19\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### **5.2.1 Building Logistic Regression Model (Baseline 1)**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg_model = LogisticRegression(\n    max_iter=1000,\n    solver='liblinear'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **5.2.2 Building Feedforward Neural Network (FNN) Model (Baseline 2)**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n# FNN\nFNN_model = Sequential([\n    Dense(64, activation = 'relu'),\n    Dropout(0.5),\n    Dense(1, activation = 'sigmoid')\n])\n\nFNN_model.compile(optimizer = Adam(learning_rate = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **5.2.3 Building Bidirectional GRU (BI-GRU) Model**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Reshape, Bidirectional, GRU\n# BI-GRU\nBI_GRU_model = Sequential([\n    Reshape((1, X_train.shape[1]), input_shape=(X_train.shape[1],)),\n    Bidirectional(GRU(64, return_sequences=False)),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nBI_GRU_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5.3 Training Model**<a id=\"20\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### **5.3.1 Training Logistic Regression Model**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ntrain_logres_model = logreg_model.fit(X_train, y_train)\n\ny_pred_logreg = logreg_model.predict(X_test)\nlogreg_accuracy = accuracy_score(y_test, y_pred_logreg)\nlogreg_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **5.3.2 Training Feedforward Neural Network (FNN) Model**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"train_FNN_model = FNN_model.fit(\n    X_train.toarray(),\n    y_train,\n    epochs = 10,\n    batch_size = 32,\n    validation_split = 0.2\n)\n\ny_pred_FNN = FNN_model.predict(X_test.toarray()).round().astype(int)\nfnn_accuracy = accuracy_score(y_test, y_pred_FNN)\nfnn_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **5.3.2 Training Bidirectional GRU (BI-GRU) Model**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"train_BI_GRU_model = BI_GRU_model.fit(\n    X_train.toarray(),\n    y_train,\n    epochs=10,\n    batch_size=32,\n    validation_split=0.2,\n    verbose=1\n)\n\ny_pred_BIGRU = BI_GRU_model.predict(X_test.toarray()).round().astype(int)\nbigru_accuracy = accuracy_score(y_test, y_pred_BIGRU)\nbigru_accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5.4 Visualizing Our Model**<a id=\"21\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### **5.4.1 Model Accuracy**<a id=\"22\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### **5.4.1.1 Model Accuracy for Logistic Regression**<a id=\"22\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### **5.4.1.2 Model Accuracy for FNN**<a id=\"22\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### **5.4.1.1 Model Accuracy for BI-GRU**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"#Training vs Validation Accuracy\nplt.figure(figsize = (6, 4))\nplt.plot(train_model.history['accuracy'])\nplt.plot(train_model.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc = 'upper left')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **5.4.2 Model Loss**<a id=\"23\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### **5.4.2.1 Model Accuracy for Logistic Regression**<a id=\"22\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### **5.4.2.2 Model Accuracy for FNN**<a id=\"22\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### **5.4.2.3 Model Accuracy for BI-GRU**<a id=\"22\"></a>","metadata":{}},{"cell_type":"code","source":"#Training vs Validation Loss\nplt.figure(figsize = (6, 4))\nplt.plot(train_model.history['loss'])\nplt.plot(train_model.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Validation'], loc = 'upper left')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5.5 Model Accuracy Evaluation**<a id=\"24\"></a>","metadata":{}},{"cell_type":"code","source":"#Evaluating Model Accuracy On Test Data\n\"\"\"Let's ensure that the model is not overfitting.\"\"\"\n\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"The Test Accuracy is: {accuracy}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Loss\nprint(f\"The Model Loss is: {loss}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n#Predictions on Test Data\ny_pred_prob = model.predict(X_test)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n#Classification Report\nclass_report = classification_report(y_test, y_pred)\nprint(class_report)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\n#Predictions on Test Data\n#y_pred_prob = model.predict(X_test)\n#y_pred = (y_pred_prob > 0.5).astype(int)\n\n#Classification Report\nclass_report = classification_report(y_test, y_pred, output_dict = True)\nclass_report_df = pd.DataFrame(class_report).transpose()\n\n#Dropping irrelevant metrics for Visualization\nclass_metrics = class_report_df.drop(['accuracy', 'macro avg', 'weighted avg'])\n\n#Classification Metrics Using Heatmap\nplt.figure(figsize = (8, 6))\nsns.heatmap(class_metrics[['precision', 'recall', 'f1-score']], annot = True, cmap = 'Reds', fmt = '.2f')\nplt.title(\"Classification Report Metrics\")\nplt.xlabel(\"Metrics\")\nplt.ylabel(\"Class\")\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see from the diagram above that the model is not overfitting.","metadata":{}},{"cell_type":"markdown","source":"#### **5.6 Saving Our Model and Vectorizer**<a id=\"25\"></a>","metadata":{}},{"cell_type":"code","source":"#Saving the Keras Model\nimport pickle\n\nwith open('tfidf_vectorizer.pkl', 'wb') as f:\n    pickle.dump(vector, f)\n\nmodel.save('toxic_comment_prediction_model.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **5.7 Testing Our Saved Model**<a id=\"26\"></a>","metadata":{}},{"cell_type":"code","source":"#Reusing The Saved Model\nimport pickle\nfrom tensorflow.keras.models import load_model\n#Import TF-IDF Vectorizer for text handling\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Loading TF-IDF Vectorizer\nwith open('/kaggle/working/tfidf_vectorizer.pkl', 'rb') as f:\n    loaded_vectorizer = pickle.load(f)\n    \n    \n#Loading The Trained Model\nloaded_model = load_model('/kaggle/working/toxic_comment_prediction_model.h5')\nnew_comments = [\n    \"You're quite a bad person at keeping to time.\",\n    \"This is a very bad service.\",\n    \"You’ve achieved so much!\",\n    \"You are very stupid and mad.\",\n]\n\n#Processing New Comments using the Loaded TF-IDF Vectorizer\nprocessed_comment = loaded_vectorizer.transform(new_comments)\n\n#Predicting using the Loaded Model\npredictions = (loaded_model.predict(processed_comment) > 0.5).astype(int)\n\n#Prediction Result\nfor comment, prediction in zip(new_comments, predictions):\n    print(f\"Comment: {comment} | Is Toxic: {bool(prediction)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}